{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# **Code of Decision Tree:**\n","\n","---\n","\n","\n","\n","\n"],"metadata":{"id":"bmgYgQUHvQ2B"}},{"cell_type":"code","source":["import numpy as np\n","from tensorflow.keras.datasets import cifar10\n","from sklearn.metrics import accuracy_score\n","from sklearn.preprocessing import StandardScaler\n","\n","# Load CIFAR-10 dataset\n","(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n","\n","# Enhanced feature extraction\n","def extract_features(images):\n","    features = []\n","    for img in images:\n","        # Convert to grayscale\n","        grayscale = np.mean(img, axis=2).astype(np.uint8)\n","\n","        # Grayscale histogram\n","        hist_gray, _ = np.histogram(grayscale, bins=16, range=(0, 256))\n","        hist_gray = hist_gray / hist_gray.sum()\n","\n","        # RGB histograms\n","        hist_r, _ = np.histogram(img[:, :, 0], bins=16, range=(0, 256))\n","        hist_g, _ = np.histogram(img[:, :, 1], bins=16, range=(0, 256))\n","        hist_b, _ = np.histogram(img[:, :, 2], bins=16, range=(0, 256))\n","        hist_r, hist_g, hist_b = hist_r / hist_r.sum(), hist_g / hist_g.sum(), hist_b / hist_b.sum()\n","\n","        # Statistical features\n","        mean_gray = np.mean(grayscale)\n","        std_gray = np.std(grayscale)\n","\n","        # Combine features\n","        features.append(np.hstack((hist_gray, hist_r, hist_g, hist_b, mean_gray, std_gray)))\n","\n","    return np.array(features)\n","\n","# Extract features for the complete dataset\n","x_train_features = extract_features(x_train)  # Use the full training set\n","x_test_features = extract_features(x_test)\n","y_train = y_train.flatten()\n","y_test = y_test.flatten()\n","\n","# Standardize features\n","scaler = StandardScaler()\n","x_train_features = scaler.fit_transform(x_train_features)\n","x_test_features = scaler.transform(x_test_features)\n","\n","# Decision tree utilities\n","def compute_entropy(y):\n","    classes, counts = np.unique(y, return_counts=True)\n","    probabilities = counts / counts.sum()\n","    entropy = -np.sum(probabilities * np.log2(probabilities + 1e-8))\n","    return entropy\n","\n","def split_dataset(X, node_indices, feature):\n","    threshold = np.median(X[node_indices, feature])  # Use median as the threshold\n","    left_indices = [i for i in node_indices if X[i, feature] <= threshold]\n","    right_indices = [i for i in node_indices if X[i, feature] > threshold]\n","    return np.array(left_indices, dtype=int), np.array(right_indices, dtype=int)\n","\n","# Code Assignment starts here\n","def compute_information_gain(X, y, node_indices, feature):\n","    left_indices, right_indices = split_dataset(X, node_indices, feature)\n","    y_node = y[node_indices]\n","    y_left = y[left_indices]\n","    y_right = y[right_indices]\n","\n","    entropy_before = compute_entropy(y_node)\n","    left_weight = len(y_left) / len(y_node)\n","    right_weight = len(y_right) / len(y_node)\n","    entropy_after = left_weight * compute_entropy(y_left) + right_weight * compute_entropy(y_right)\n","\n","    information_gain = entropy_before - entropy_after\n","    return information_gain\n","\n","def get_best_split(X, y, node_indices):\n","    num_features = X.shape[1]\n","    best_feature = -1\n","    max_information_gain = -float('inf')\n","\n","    if len(np.unique(y[node_indices])) == 1:\n","        return best_feature\n","\n","    for feature in range(num_features):\n","        information_gain = compute_information_gain(X, y, node_indices, feature)\n","        if information_gain > max_information_gain:\n","            max_information_gain = information_gain\n","            best_feature = feature\n","\n","    return best_feature\n","# Code Assignment stops here\n","\n","def build_tree_recursive(X, y, node_indices, max_depth, current_depth):\n","    # Stop recursion if max depth is reached or no samples are available\n","    if current_depth == max_depth or len(node_indices) == 0:\n","        return np.bincount(y[node_indices]).argmax() if len(node_indices) > 0 else -1\n","\n","    # Find the best feature to split\n","    best_feature = get_best_split(X, y, node_indices)\n","    if best_feature == -1:\n","        return np.bincount(y[node_indices]).argmax()\n","\n","    # Split dataset\n","    left_indices, right_indices = split_dataset(X, node_indices, best_feature)\n","\n","    # Recursively build left and right subtrees\n","    left_child = build_tree_recursive(X, y, left_indices, max_depth, current_depth + 1)\n","    right_child = build_tree_recursive(X, y, right_indices, max_depth, current_depth + 1)\n","\n","    # Return the tree structure as a tuple\n","    return (best_feature, left_child, right_child)\n","\n","# Build the decision tree\n","node_indices = np.arange(len(x_train_features))\n","decision_tree = build_tree_recursive(x_train_features, y_train, node_indices, max_depth=6, current_depth=0)  # Increased max_depth\n","\n","# Function to predict using the decision tree\n","def predict(tree, x):\n","    while isinstance(tree, tuple):  # Traverse the tree until reaching a leaf\n","        feature, left_child, right_child = tree\n","        threshold = np.median(x_train_features[:, feature])\n","        if x[feature] <= threshold:\n","            tree = left_child\n","        else:\n","            tree = right_child\n","    return tree  # Return the class label at the leaf\n","\n","# Predict and evaluate\n","y_test_pred = [predict(decision_tree, x_test_features[i]) for i in range(len(x_test_features))]\n","test_accuracy = accuracy_score(y_test, y_test_pred)\n","print(f\"Test Accuracy: {test_accuracy * 100:.2f}%\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7iqc5LvGr1HO","executionInfo":{"status":"ok","timestamp":1734630621333,"user_tz":-60,"elapsed":138484,"user":{"displayName":"Yixuan Li","userId":"03838865976624798603"}},"outputId":"45f2eca4-bd4f-4929-9e1f-76c380fe1c03"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n","\u001b[1m170498071/170498071\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 0us/step\n","Test Accuracy: 23.65%\n"]}]},{"cell_type":"markdown","source":["# **Code of Random Forest:**\n","\n","---\n","\n"],"metadata":{"id":"_-PnWC8hvskW"}},{"cell_type":"code","source":["from sklearn.ensemble import RandomForestClassifier\n","from sklearn.metrics import accuracy_score\n","from sklearn.decomposition import PCA\n","\n","# flatten dataset\n","x_train_flatten = x_train.reshape(x_train.shape[0], -1)\n","x_test_flatten = x_test.reshape(x_test.shape[0], -1)\n","\n","scaler = StandardScaler()\n","x_train_scaled = scaler.fit_transform(x_train_flatten)\n","x_test_scaled = scaler.transform(x_test_flatten)\n","\n","# use PCA\n","pca = PCA(n_components=0.95)\n","x_train_pca = pca.fit_transform(x_train_scaled)\n","x_test_pca = pca.transform(x_test_scaled)\n","\n","# set parameters\n","clf_rf = RandomForestClassifier(\n","    n_estimators=200,\n","    max_depth=25,\n","    max_features='sqrt',\n","    min_samples_split=5,\n","    min_samples_leaf=3,\n","    random_state=42\n",")\n","clf_rf.fit(x_train_pca, y_train)\n","\n","y_test_rf_pred = clf_rf.predict(x_test_pca)\n","test_rf_accuracy = accuracy_score(y_test, y_test_rf_pred)\n","print(f\"Test Accuracy with RF: {test_rf_accuracy * 100:.2f}%\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zoz2Xxl-5RZK","executionInfo":{"status":"ok","timestamp":1734633061925,"user_tz":-60,"elapsed":511220,"user":{"displayName":"Yixuan Li","userId":"03838865976624798603"}},"outputId":"e66437ca-11ec-4a99-a8ac-937542345be8"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Test Accuracy with All Features (RF): 46.86%\n"]}]}]}